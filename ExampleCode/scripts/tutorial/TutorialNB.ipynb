{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bc2f83-2bf3-4108-acdc-3428f32b91b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5997/2034768389.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c519edef-ca89-4590-9ca0-9bbe8f3df005",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/slagarp/exjobb/ExampleCode/scripts/tutorial\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_common=os.path.dirname(os.path.abspath(''))+\"/common/\";\n",
    "sys.path.append(path_common) #Adds common path to import the python files\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a3a741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/slagarp/exjobb/ExampleCode/scripts/tutorial',\n",
       " '/usr/lib/python310.zip',\n",
       " '/usr/lib/python3.10',\n",
       " '/usr/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/slagarp/.local/lib/python3.10/site-packages',\n",
       " '/usr/local/lib/python3.10/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/home/slagarp/exjobb/ExampleCode/scripts/common/']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f542017-92b5-4b0b-a528-10a679b44607",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset Generation\n",
    "The functions to manipulate the dataset allow for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aee4c905-a782-4f36-876b-24689f93ba43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Train  Test\n",
      "000   5769   315\n",
      "001   8167   838\n",
      "010   2418   101\n",
      "011  14632  1977\n",
      "100    919    83\n",
      "101   7551   427\n",
      "110   6731   606\n",
      "111   6979   665\n",
      "         Train       Test\n",
      "000  94.822485   5.177515\n",
      "001  90.694059   9.305941\n",
      "010  95.990472   4.009528\n",
      "011  88.096815  11.903185\n",
      "100  91.716567   8.283433\n",
      "101  94.647781   5.352219\n",
      "110  91.740493   8.259507\n",
      "111  91.300366   8.699634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slagarp/exjobb/ExampleCode/scripts/common/DatasetFuncs.py:137: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '94.8224852071006' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  samples_perc.iloc[i,0]=100*samples_dist.iloc[i,0]/(samples_dist.iloc[i,0]+samples_dist.iloc[i,1]);\n",
      "/home/slagarp/exjobb/ExampleCode/scripts/common/DatasetFuncs.py:138: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '5.177514792899408' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  samples_perc.iloc[i,1]=100*samples_dist.iloc[i,1]/(samples_dist.iloc[i,0]+samples_dist.iloc[i,1]);\n"
     ]
    }
   ],
   "source": [
    "from DatasetFuncs import allDataset_loader,dataset_split,show_porcentages,show_partition_nanopores\n",
    "\n",
    "data_folder=\"../../ext/QuipuData/\"; #path where the datasets are/will be stored\n",
    "allDatasets=allDataset_loader(data_folder) #Generates the processed df and saves it. If it is already in memory\n",
    "#then it loads it. One can specify cut=False to obtain the uncut traces (if not they are cut at 700 samples as was used in quipunet)\n",
    "\n",
    "trainSet,testSet=dataset_split(allDatasets)#Divides in train and test, in a way that the test set is within a range of\n",
    "# % of samples, and that nanopores used on train set are not used in train for the same barcode.\n",
    "#NOTE: This is to generate random test sets, if you want to use the ones from quipu should be coded!\n",
    "samples_perc=show_porcentages(trainSet,testSet) #Shows number of samples for train and test set (and then percentages), for each barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3b6fe0-8a72-4d2a-bc14-453f7bb8d007",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   barcode  nanopore  size\n",
      "0      000         7  2172\n",
      "1      000         8  3498\n",
      "2      000      1017    99\n",
      "3      001         8  1737\n",
      "4      001        26   502\n",
      "5      001        27  3583\n",
      "6      001        28   923\n",
      "7      001        29  1255\n",
      "8      001      1053   167\n",
      "9      010         7  1021\n",
      "10     010         9   629\n",
      "11     010        10   768\n",
      "12     011         6  1332\n",
      "13     011         7   702\n",
      "14     011        12   577\n",
      "15     011        31   899\n",
      "16     011        33  2192\n",
      "17     011        35   483\n",
      "18     011        36  1780\n",
      "19     011        37  1583\n",
      "20     011        38  1229\n",
      "21     011        39  1735\n",
      "22     011        40  2120\n",
      "23     100         7   288\n",
      "24     100        13   631\n",
      "25     101         8  1240\n",
      "26     101        26  1764\n",
      "27     101        27   924\n",
      "28     101        29  3494\n",
      "29     101      1662   129\n",
      "30     110         7  1161\n",
      "31     110         8  2702\n",
      "32     110        13  1309\n",
      "33     110        14  1559\n",
      "34     111         7   735\n",
      "35     111         8  2866\n",
      "36     111         9   677\n",
      "37     111        15  1092\n",
      "38     111        32   346\n",
      "39     111        34  1263\n",
      "   barcode  nanopore  size\n",
      "0      000         6   253\n",
      "1      000      1014    62\n",
      "2      001         7   838\n",
      "3      010      1159   101\n",
      "4      011        11   362\n",
      "5      011        32   344\n",
      "6      011        41  1271\n",
      "7      100      1933    83\n",
      "8      101        30   427\n",
      "9      110        12   606\n",
      "10     111        14   665\n"
     ]
    }
   ],
   "source": [
    "#With this code we show which is the nanopores used for this train and test dataset\n",
    "print(show_partition_nanopores(trainSet))\n",
    "print(show_partition_nanopores(testSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a28a9c-e140-4e85-8672-06973e0939c0",
   "metadata": {},
   "source": [
    "# Testing reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a73b1c-b19a-4a9e-b9bb-31c1d570d94c",
   "metadata": {},
   "source": [
    "With this function we can run the training of quipu how it was done in their notebook (with n epochs=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad775c05-80d0-4aad-9bbd-21b7cae39761",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n",
      "  prep time: 1.4 sec   train time: 52.8 sec\n",
      "  loss: 1.985   acc: 0.1952   val_acc: 0.2167\n",
      "=== Epoch: 1 ===\n",
      "  prep time: 1.4 sec   train time: 51.2 sec\n",
      "  loss: 1.872   acc: 0.2248   val_acc: 0.2321\n",
      "       [ loss , accuracy ]\n",
      "Train: [1.8756051063537598, 0.21382546424865723]\n",
      "Validation  : [1.8405187129974365, 0.23213452100753784]\n",
      "Test : [2.393502712249756, 0.11225364357233047]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.21382546424865723, 0.23213452100753784, 0.11225364357233047)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ModelTrainer import ModelTrainer\n",
    "from ModelFuncs import get_quipu_model\n",
    "mt=ModelTrainer()\n",
    "model=get_quipu_model();\n",
    "mt.quipu_def_train(model,n_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161aa8d-725d-4ca1-8c11-e728f17e1f29",
   "metadata": {},
   "source": [
    "This code replicates the training as it is done in the github, but for a random test dataset. This is just to show, but the training I recommend doing is the one which will be shown in the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6d6a9-155d-49e3-aaa2-fcc9e5126e48",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9211c29a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 18:33:42.240727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-23 18:33:42.258591: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 18:33:42.258602: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 18:33:42.259095: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-23 18:33:42.262239: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-23 18:33:42.646715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-02-23 18:33:44.146559: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.157337: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.157369: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.158171: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.158209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.158228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.314678: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.314719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.314724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-23 18:33:44.314750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-23 18:33:44.314766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6687 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 18:33:48.868385: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-02-23 18:33:48.935994: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-23 18:33:49.100408: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-23 18:33:49.837107: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd82ce83710 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-23 18:33:49.837131: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2024-02-23 18:33:49.840059: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708709629.894201    6142 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 67s 81ms/step - loss: 1.4420 - accuracy: 0.5117\n",
      "Validation ds:\n",
      "16/16 [==============================] - 4s 165ms/step - loss: 1.8999 - accuracy: 0.4225\n",
      "  prep time: 2.7 sec   train time: 71.5 sec\n",
      "=== Epoch: 1 ===\n",
      "141/782 [====>.........................] - ETA: 53s - loss: 1.2020 - accuracy: 0.5951"
     ]
    }
   ],
   "source": [
    "from ModelTrainer import ModelTrainer\n",
    "from ModelFuncs import create_fcn_model #get_resnet_model\n",
    "mt=ModelTrainer(n_epochs_max=10); #use_weights option weights classes instead of oversampling. track_losses is used to save each result for every cross-validation\n",
    "#(model, modelinfo)=get_resnet_model();\n",
    "model = create_fcn_model((None, 1), 8);\n",
    "mt.train_es(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e0aae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
